---
title: 'Data import, quality control and normalization for the Macosko dataset'
author: "Koen Van den Berge and Jeroen Gilis"
date: "20/11/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Preamble: installation of Bioconductor libraries

```{r}
# install BiocManager package if not installed yet.
# BiocManager is the package installer for Bioconductor software.
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

# install packages if not yet installed.
pkgs <- c("SingleCellExperiment",
          "ExperimentHub",
          "edgeR",
          "DropletUtils", 
          "scRNAseq", 
          "scater", 
          "scuttle", 
          "scran", 
          "BiocSingular", 
          "scDblFinder")
notInstalled <- pkgs[!pkgs %in% installed.packages()[,1]]
if(length(notInstalled) > 0){
  BiocManager::install(notInstalled)
}
```

# The Macosko dataset

In this workshop session, we will preprocess the single-cell RNA-seq dataset
from the publication by Macosko *et al.*, Cell 161, 1202–1214 from 2015
[link](https://doi.org/10.1016/j.cell.2015.05.002). This is the manuscript in
which the droplet scRNA-seq technology **drop-seq** was introduced.
Six years after the original publication, drop-seq is still one of the most 
commonly adopted scRNA-seq protocols, as evidenced by the
large number of citations for Macosko *et al.* 
(4.303 citations at November 3, 2021).

The basic idea behind the drop-seq protocol can be taken from the graphical
abstract of the publication.

```{r, echo=FALSE}
knitr::include_graphics("macosko_graphicalAbstract.jpeg")
```

The success of drop-seq can be explained by the following distinguishing 
features:

- Unique molecular identifier (UMI) technology. By working with UMIs, one count
corresponds to one observed mRNA molecule present in the cell. Thanks to the use
of UMI barcodes, PCR artifacts are reduced.

- Scalability: microfluidics technology allows for performing the scRNA-seq
reaction inside droplet that each contain a single cell, for ±10.000 droplets
simultaneously.

- Cost: the experiment costs around 6.5 cents (USD) per cell.

- Speed: The very large dataset that we will be working with today was 
generated in an experiment that took only 4 days.

In this particular experiment, Macosko *et al.* sequenced 49.300 cells from the
mouse retina, identifying 39 transcriptionally distinct cell populations. The
experiment was performed in 7 batches.

# Data availability

## SRA

The sequencing read archive ([SRA](https://www.ncbi.nlm.nih.gov/sra)) is the 
largest publicly available repository of high throughput sequencing data.
The data are stored by the National Center for Biotechnology Information (NCBI) 
services and multiple cloud storage providers. From this website "raw" 
sequencing data can be retrieved. In practice, these are usually `.sra` files,
which can be downloaded and converted into FASTQ files using functions
from the `sratoolkit` software. 

For our dataset, the FASTQ data can be retrieved from this 
[link](https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA267857&o=acc_s%3Aa).
The data are stored as one file per sequencing batch, with each file 
approximately 20Gb. As such, it will be unfeasible to download and process these
FASTQ files in this practical session.

In stead, for demonstrative purposes, we have taken a subsample of the FASTQ
file for the first sequencing batch for you to work with. On this subsample, 
we may perform all the tasks that we would have performed on the full dataset.
The steps that are required for downloading and quantifying drop-seq data
can be found on our companion github
[page](https://github.com/statOmics/singleCellCourse/blob/master/lab1_preprocessing/preprocessDropseq.sh)

## GEO

The dataset from Macosko *et al.* was also uploaded by the authors on the 
Gene Expression Omnibus (GEO) platform under accession number 
[GSE63472](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE63472), 
from which the following information can be retrieved:

1. *GSE63472_RAW.tar*, a 90.6Gb object that contains the "raw data" for the
experiment. In the scRNA-seq context, FASTQ files are often considered the raw
data format.

2. *GSE63472_P14Retina_merged_digital_expression.txt.gz*, a 50.7Mb matrix that
stores the gene expression values for each cell. These values are integer 
counts, that did not undergo any type of preprocessing or normalization.

3. *GSE63472_mm10_reference_metadata.tar.gz*, a 862.9Mb compressed folder
containing information on the reference genome to which the scRNA-seq reads
were aligned (see theory slides).

4. *GSE63472_P14Retina_logDGE.txt.gz*, a 316.8Mb compressed text file, not clear
what it contains (results from a differential gene expression analysis, but
with log-transformation, so log-fold changes maybe?).

As such, by downloading *GSE63472_P14Retina_merged_digital_expression.txt.gz*,
we avoid re-quantifying the data, i.e., the translation from reads from the
FASTQ files into a gene-level expression values for each cell. 

One issue that often arises from data downloaded from GEO, is that there is no
strict requirements for which data should be included in the upload by the 
authors. As such, from my personal experience, it can often be the case that
important information like metadata are missing, or the content of the submitted
files is unclear. Even if all the required data are available, as is the case 
here, we would still need to piece all the information together from different 
files and file formats before we can use them.

## ExperimentHub

The Bioconductor *ExperimentHub* web resource, which can be accessed using the 
[ExperimentHub](https://bioconductor.org/packages/release/bioc/html/ExperimentHub.html) 
R package, provides a central location where curated data from experiments, 
publications or training courses can be accessed. While it contains far less
datasets than the SRA or GEO (4965 records to date), these datasets all follow 
the tidy data format of Bioconductor. Note that the ExperimentHub contains 
several types of data, like bulk and single-cell transcriptomics data, 
microarrays and more.

The Macosko dataset is available from ExperimentHub and can be accessed as
follows:

```{r, message=FALSE, warning=FALSE}
library(ExperimentHub)
edb <- ExperimentHub()

edb[grep("Macosko", edb$title)] # find accession number (can be inefficient)

edb_counts <- edb[["EH2690"]]
edb_counts[1:5,1:5]

edb_coldata <- edb[["EH2691"]]
edb_coldata[1:5,]

rm(edb, edb_counts, edb_coldata)
```

## scRNASeq

In addition to ExperimentHub, Bioconductor provides the package
[scRNAseq](https://bioconductor.org/packages/release/data/experiment/html/scRNAseq.html).
This package provides an even more user-friendly client to access (only) 
scRNA-seq datasets from the ExperimentHub web resource. Data retrieved using the
scRNAseq package are stored as user-friendly `SingleCellExperiment` objects, 
with the expression data, gene-level information, cell-level information and 
experiment metadata all in place in one data object. The scRNA-seq package
currently holds 61 datasets, including the data from *Macosko et al.*:

```{r, message=FALSE, warning=FALSE}
library(scRNAseq)
scRNAseq::MacoskoRetinaData()
```

# Import data

The `scRNAseq` package provides convenient access to several datasets. See the [package Bioconductor page](http://bioconductor.org/packages/release/data/experiment/html/scRNAseq.html) 
for more information.

```{r}
# Code below might ask you to create an ExperimentHub directory. 
# Type 'yes' and hit Enter, to allow this.
suppressPackageStartupMessages(library(scRNAseq))
sce <- MacoskoRetinaData()
```

# A `SingleCellExperiment` object

```{r}
sce
```

## Accessing data from a `SingleCellExperiment` object

Please see [Figure 4.1 in OSCA](http://bioconductor.org/books/release/OSCA/data-infrastructure.html) 
for an overview of a `SingleCellExperiment` object.

```{r}
# Data: assays
assays(sce)
assays(sce)$counts[1:5, 1:5]

# Feature metadata: rowData
rowData(sce) # empty for now

# Cell metadata: colData
colData(sce)

# Reduced dimensions: reducedDims
reducedDims(sce) # empty for now
```

## Creating a new `SingleCellExperiment` object

```{r}
sceNew <- SingleCellExperiment(assays = list(counts = assays(sce)$counts))
sceNew

rm(sceNew)
```

## Storing (meta)data in a `SingleCellExperiment` object

```{r}
fakeGeneNames <- paste0("gene", 1:nrow(sce))
rowData(sce)$fakeName <- fakeGeneNames
head(rowData(sce))
# Remove again by setting to NULL
rowData(sce)$fakeName <- NULL

assays(sce)$logCounts <- log1p(assays(sce)$counts)
assays(sce)
assays(sce)$logCounts[1:5, 1:5]
assays(sce)$logCounts <- NULL
```

# Obtaining and including rowData

The `rowData` slot of a `SingleCellExperiment` object allows for storing 
information on the features, i.e. the genes, in a dataset. In our object,
the `rowData` slot is empty.

```{r}
rowData(sce)
```

As such, the only information we have on the genes are their names, which can
be retrieved as the `rownames` of the expression matrix.

```{r}
head(rownames(sce))
```

These are the gene names (symbols). Note that it may be useful to include 
additional information in the `rowData` slot. For instance, we may want to 
store:

- Unambiguous gene identifiers (e.g. from ENSEMBL)
- On which chromosome the gene is located
- Gene length (genomic start position and end position)
- Others...

```{r, message=FALSE, warning=FALSE}
library("biomaRt")

ensembl75 <- useEnsembl(biomart = 'genes', 
                        dataset = 'mmusculus_gene_ensembl',
                        version = 75)

head(listAttributes(ensembl75)) # potential info to extract

geneInfo <- getBM(attributes = c("ensembl_gene_id", # ENSEMBL unambiguous identifier
                                 "mgi_symbol", # Gene symbol (to link with SCE rownames),
                                 "chromosome_name", # On which chromosome
                                 "start_position", # Start position
                                 "end_position"),# End position
                  mart = ensembl75)

head(geneInfo)
```

```{r}
geneInfo$mgi_symbol_upper <- toupper(geneInfo$mgi_symbol) 
# match between gene info and rownames

sum(rownames(sce) %in% geneInfo$mgi_symbol_upper)
sum(!rownames(sce) %in% geneInfo$mgi_symbol_upper) # lost in conversion :(

rowData(sce) <- geneInfo[match(rownames(sce),geneInfo$mgi_symbol_upper),]
```

```{r}
head(rowData(sce))
```

# Filtering non-informative genes

```{r}
library(edgeR)

# A very simple strategy: remove all genes that are expressed in less than 10
# out of 49300 cells -> note that this a very lenient filtering criterium
keep <- rowSums(assays(sce)$counts > 0) > 10
table(keep)

# Dedicated functions for filtering out lowly expressed genes exist.
# For instance, edgeR allows to retain genes that have an expression of at 
# least 1 in at least 10 cells (min.prop = 20% of the cells from the smallest 
# group). The sum of the counts over all cells must be higher than 
# min.total.count = 15.

# Slow (more than 1min) -> do not run
# keep2 <- filterByExpr(sce,
#                       group = sce$cluster,
#                       min.count = 1,
#                       min.total.count = 15,
#                       min.prop = 0.2)
# table(keep2)

# continue with filtering 1
sce <- sce[keep,]
```

# Quality control

Distinguishing features of low-quality cells (from the
[OSCA](http://bioconductor.org/books/3.14/OSCA.basic/quality-control.html) 
book):

1. The library size is defined as the total sum of counts across all 
relevant features for each cell. Here, we will consider the relevant features 
to be the endogenous genes. Cells with small library sizes are of low quality 
as the RNA has been lost at some point during library preparation, either due 
to cell lysis or inefficient cDNA capture and amplification.

2. The number of expressed features in each cell is defined as the number of 
endogenous genes with non-zero counts for that cell. Any cell with very 
few expressed genes is likely to be of poor quality as the diverse transcript 
population has not been successfully captured.

3. We sometimes have spike-in (ERCC) transcripts available.
The proportion of reads mapped to spike-in transcripts is calculated relative 
to the total count across all features (including spike-ins) for each cell. 
As the same amount of spike-in RNA should have been added to each cell, 
any enrichment in spike-in counts is symptomatic of loss of endogenous RNA.

4. In the absence of spike-in transcripts, the proportion of reads mapped 
to genes in the mitochondrial genome can be used. High proportions are 
indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), 
presumably because of loss of cytoplasmic RNA from perforated cells. 
The reasoning is that, in the presence of modest damage, the holes in the 
cell membrane permit efflux of individual transcript molecules but are too 
small to allow mitochondria to escape, leading to a relative enrichment of 
mitochondrial transcripts. For single-nuclei RNA-seq experiments, high 
proportions are also useful as they can mark cells where the cytoplasm has 
not been successfully stripped.

## Calculate QC variables

```{r, message=FALSE, warning=FALSE}
library(scater)

# check ERCC spike-in transcripts
sum(grepl("^ERCC-", rownames(sce))) # no spike-in transcripts available

# check mitochondrial genes
sum(rowData(sce)$chromosome_name=="MT",na.rm = TRUE) # 28 mitochondrial genes
sum(grepl("^MT-", rownames(sce))) # alternatively
is.mito <- grepl("^MT-", rownames(sce))

df <- perCellQCMetrics(sce, subsets=list(Mito=is.mito)) 
# add QC variables to sce object

colData(sce) <- cbind(colData(sce), df)

# the QC variables have now been added to the colData of our SCE object.
head(colData(sce))
```

## Exploratory data analysis

In the figure below, we see that several cells have a very low number of 
expressed genes, and where most of the molecules are derived from 
mitochondrial genes. This indicates likely damaged cells, presumably because 
of loss of cytoplasmic RNA from perforated cells, so we should remove these for 
the downstream analysis.

```{r}
# Number of genes vs library size
plotColData(sce, x = "sum", y="detected", colour_by="cluster") 

# Mitochondrial genes
plotColData(sce, x = "detected", y="subsets_Mito_percent")
```

## QC using adaptive thresholds

Below, we remove cells that are outlying with respect to

 1. A low sequencing depth (number of UMIs);
 2. A low number of genes detected;
 3. A high percentage of reads from mitochondrial genes.
 
 Here we will remove cells for QC based on **adaptive thresholds** related to
 the three points from above. Adaptive trhesholds are used as opposed to
 **fixed thresholds**. 
 
With fixed thresholds, we use fixed cut-off values for each cell to pass QC, 
e.g., we might consider cells to be low quality if they have library sizes 
below 100,000 reads; express fewer than 5,000 genes; have spike-in proportions
above 10%; or have mitochondrial proportions above 10%.

With adaptive thresholds, we assume that most of the dataset consists of 
high-quality cells. We then identify cells that are outliers for the various 
QC metrics, based on the median absolute deviation (MAD) from the median value
of each metric across all cells. By default, we consider a value to be anoutlier 
if it is more than 3 MADs from the median in the “problematic” direction. This 
is loosely motivated by the fact that such a filter will retain 99% of 
non-outlier values that follow a normal distribution. We demonstrate adopting
adaptive thresholds on the Macosko dataset:

```{r}
lowLib <- isOutlier(df$sum, type="lower", log=TRUE)
lowFeatures <- isOutlier(df$detected, type="lower", log=TRUE)
highMito <- isOutlier(df$subsets_Mito_percent, type="higher")

table(lowLib)
table(lowFeatures)
table(highMito)

discardCells <- (lowLib | lowFeatures | highMito)
table(discardCells)
colData(sce)$discardCells <- discardCells

# visualize cells to be removed
plotColData(sce, x = "sum", y="detected", colour_by="discardCells")
plotColData(sce, x = "detected", y="subsets_Mito_percent", colour_by = "discardCells")
```

We removed a total of $3423$ cells, most of which because of an outlyingly high 
percentage of reads from mitochondrial genes.

## Identifying and removing empty droplets

Note that the removal of cells with low sequencing depth using the adaptive 
threshold procedure above is a way of removing empty droplets. 
Other approaches are possible, e.g., removing cells by statistical testing 
using `emtpyDrops`. This does require us to specify a lower bound on the total 
number of UMIs, below which all cells are considered to correspond to empty 
droplets. This lower bound may not be trivial to derive, but the `barcodeRanks`
function can be useful to identify an elbow/knee point.

```{r, message=FALSE, warning=FALSE}
library(DropletUtils)
bcrank <- barcodeRanks(counts(sce))

# Only showing unique points for plotting speed. Duplicated ranks are a 
# consequence of ties in the ranks, i.e., when cells have an equal library size.
sum(duplicated(bcrank$rank))
uniq <- !duplicated(bcrank$rank)

plot(bcrank$rank[uniq], bcrank$total[uniq], log="xy",
    xlab="Rank", ylab="Total UMI count", cex.lab=1.2)
abline(h=metadata(bcrank)$inflection, col="darkgreen", lty=2)
abline(h=metadata(bcrank)$knee, col="dodgerblue", lty=2)
abline(h=350, col="orange", lty=2) # picked visually myself
legend("topright", 
       legend=c("Inflection", "Knee", "Empirical knee point"), 
       col=c("darkgreen", "dodgerblue", "orange"), 
       lty=2, 
       cex=1.2)

set.seed(100)
limit <- 350   
all.out <- emptyDrops(counts(sce), lower=limit, test.ambient=TRUE)
# p-values for cells with total UMI count under the lower bound.
hist(all.out$PValue[all.out$Total <= limit & all.out$Total > 0],
    xlab="P-value", main="", col="grey80", breaks=20)

# but note that it would remove a very high number of cells
length(which(all.out$FDR <= 0.001))

# so we stick to the more lenient adaptive filtering strategy
# remove cells identified using adaptive thresholds
sce <- sce[, !colData(sce)$discardCells]
```

## Identifying and removing doublets

We will use 
[scDblFinder](https://bioconductor.org/packages/3.14/bioc/html/scDblFinder.html) 
to detect doublet cells.

```{r, message=FALSE, warning=FALSE}
## perform doublet detection
library(scDblFinder)
```

```{r}
set.seed(211103)
sampleID <- unlist(lapply(strsplit(colData(sce)$cell.id, split="_"), "[[", 1))
table(sampleID)
sce <- scDblFinder(sce, 
                   returnType="table",
                   samples = factor(sampleID))
table(sce$scDblFinder.class)


## visualize these scores
## explore doublet score wrt original cluster labels
boxplot(log1p(sce$scDblFinder.score) ~ factor(colData(sce)$cluster, exclude=NULL))

tab <- table(sce$scDblFinder.class, sce$cluster, 
      exclude=NULL)
tab
t(t(tab) / colSums(tab))

barplot(t(t(tab) / colSums(tab))[2,],
        xlab = "Cluster", ylab = "Fraction of doublets")

range(sce$scDblFinder.score[sce$scDblFinder.class  == "doublet" & sampleID == "r1"])
range(sce$scDblFinder.score[sce$scDblFinder.class  == "singlet" & sampleID == "r1"])
```

```{r}
# remove doublets
sce <- sce[,!sce$scDblFinder.class == "doublet"]
```

# Normalization

For normalization, the size factors $s_i$ computed here are simply scaled 
library sizes:
\[ N_i = \sum_g Y_{gi} \]
\[ s_i = N_i / \bar{N}_i \]

```{r}
sce <- logNormCounts(sce)

# note we also returned log counts: see the additional logcounts assay.
sce

# you can extract size factors using
sf <- librarySizeFactors(sce)
mean(sf) # equal to 1 due to scaling.
plot(x= log(colSums(assays(sce)$counts)), 
     y=sf)
```

From the OSCA book:
Alternatively, we may use more sophisticated approaches for variance stabilizing 
transformations in genomics data, e.g., `DESeq2` or `sctransform`. These aim 
to remove the mean-variance trend more effectively than the simpler 
transformations mentioned above, though it could be argued whether this is 
actually desirable. For low-coverage scRNA-seq data, there will always be a 
mean-variance trend under any transformation, for the simple reason that the 
variance must be zero when the mean count is zero. These methods also face the 
challenge of removing the mean-variance trend while preserving the interesting 
component of variation, i.e., the log-fold changes between subpopulations; this 
may or may not be done adequately, depending on the aggressiveness of 
the algorithm.

In practice, the log-transformation is a good default choice due to its 
simplicity and interpretability, and is what we will be using for all 
downstream analyses.

--- end lab session 1 ---




